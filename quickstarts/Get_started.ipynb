{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtVOlmDSHmh4"
      },
      "source": [
        "##### Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install kagglehub scikit-learn scikit-image opencv-python scipy joblib matplotlib seaborn tqdm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPTmuiZqBIV9",
        "outputId": "c879beb7-b37b-4cf7-983b-31cc503e2dbe"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (0.25.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kagglehub) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub) (2.32.4)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (3.6.1)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (11.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (2.37.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (2025.12.12)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.12/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import joblib\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report, roc_auc_score\n",
        ")\n",
        "from skimage.feature import local_binary_pattern, hog\n",
        "from scipy import fftpack\n",
        "import kagglehub"
      ],
      "metadata": {
        "id": "3j9LrRBECK1Z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths\n",
        "WORK_DIR = '/content/deepfake_detection'\n",
        "DATA_DIR = f'{WORK_DIR}/data'\n",
        "MODELS_DIR = f'{WORK_DIR}/models'\n",
        "RESULTS_DIR = f'{WORK_DIR}/results'\n",
        "\n",
        "\n",
        "# Create directories\n",
        "for directory in [WORK_DIR, DATA_DIR, MODELS_DIR, RESULTS_DIR]:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "# Dataset configuration\n",
        "IMAGE_SIZE = (256, 256)\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# NOTE: Dataset is already split into Train/Validation/Test\n",
        "# use the pre-existing splits\n",
        "\n",
        "# Model configuration\n",
        "RF_N_ESTIMATORS = 100\n",
        "RF_MAX_DEPTH = 20\n",
        "USE_FEATURE_SELECTION = True\n",
        "N_FEATURES_TO_SELECT = 50\n",
        "\n",
        "print(\"✅ Configuration complete!\")\n",
        "print(f\"\\nWorking directory: {WORK_DIR}\")\n",
        "print(f\"Image size: {IMAGE_SIZE}\")\n",
        "print(f\"Random Forest trees: {RF_N_ESTIMATORS}\")\n",
        "print(f\"Feature selection: {USE_FEATURE_SELECTION}\")\n",
        "if USE_FEATURE_SELECTION:\n",
        "    print(f\"  Features to select: {N_FEATURES_TO_SELECT}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9UfFZ4GCPSl",
        "outputId": "7e629228-fac1-4504-e868-b15135d988bb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Configuration complete!\n",
            "\n",
            "Working directory: /content/deepfake_detection\n",
            "Image size: (256, 256)\n",
            "Random Forest trees: 100\n",
            "Feature selection: True\n",
            "  Features to select: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "print(\"Please upload your kaggle.json file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Setup Kaggle credentials\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download dataset\n",
        "print(\"Downloading dataset...\")\n",
        "dataset_path = kagglehub.dataset_download(\"manjilkarki/deepfake-and-real-images\")\n",
        "print(f\"Dataset downloaded to: {dataset_path}\")\n",
        "\n",
        "# Explore dataset structure\n",
        "print(\"\\nDataset contents:\")\n",
        "for item in os.listdir(dataset_path):\n",
        "    item_path = os.path.join(dataset_path, item)\n",
        "    if os.path.isdir(item_path):\n",
        "        num_files = len(os.listdir(item_path))\n",
        "        print(f\"   {item}/ ({num_files} files)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "2GK8Pf4dCWUp",
        "outputId": "ea41b383-4813-4eaf-eabc-731d57b1254b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your kaggle.json file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-458ab09a-7ba0-4c65-a6a5-5ae0d534035a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-458ab09a-7ba0-4c65-a6a5-5ae0d534035a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Downloading dataset...\n",
            "Using Colab cache for faster access to the 'deepfake-and-real-images' dataset.\n",
            "Dataset downloaded to: /kaggle/input/deepfake-and-real-images\n",
            "\n",
            "Dataset contents:\n",
            "   Dataset/ (3 files)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import time\n",
        "\n",
        "class FastFeatureExtractor:\n",
        "\n",
        "\n",
        "    def extract_features(self, image):\n",
        "        features = []\n",
        "\n",
        "        # === COLOR FEATURES (Fast) ===\n",
        "        for channel in cv2.split(image):\n",
        "            features.extend([\n",
        "                np.mean(channel),\n",
        "                np.std(channel),\n",
        "                np.min(channel),\n",
        "                np.max(channel),\n",
        "                np.percentile(channel, 25),\n",
        "                np.percentile(channel, 75),\n",
        "            ])\n",
        "\n",
        "        # Convert to grayscale\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # === GRAYSCALE STATISTICS ===\n",
        "        features.extend([\n",
        "            np.mean(gray),\n",
        "            np.std(gray),\n",
        "            np.var(gray),\n",
        "            np.median(gray),\n",
        "        ])\n",
        "\n",
        "        # === FREQUENCY FEATURES (IMPORTANT!) ===\n",
        "        # DCT - catches compression artifacts (KEY for deepfakes)\n",
        "        dct = cv2.dct(np.float32(gray))\n",
        "        dct_low = dct[:32, :32]  # Use smaller region for speed\n",
        "        features.extend([\n",
        "            np.mean(dct_low),\n",
        "            np.std(dct_low),\n",
        "            np.max(dct_low),\n",
        "            np.min(dct_low),\n",
        "            np.median(dct_low),\n",
        "        ])\n",
        "\n",
        "        # FFT - frequency domain analysis\n",
        "        f_transform = np.fft.fft2(gray)\n",
        "        f_shift = np.fft.fftshift(f_transform)\n",
        "        magnitude = np.abs(f_shift)\n",
        "\n",
        "        # Radial frequency features\n",
        "        center = np.array(magnitude.shape) // 2\n",
        "        y, x = np.ogrid[:magnitude.shape[0], :magnitude.shape[1]]\n",
        "        r = np.sqrt((x - center[1])**2 + (y - center[0])**2)\n",
        "\n",
        "        for radius in [20, 50, 80]:\n",
        "            mask = (r >= radius-10) & (r < radius+10)\n",
        "            if np.sum(mask) > 0:\n",
        "                features.append(np.mean(magnitude[mask]))\n",
        "\n",
        "        # === TEXTURE FEATURES (IMPORTANT!) ===\n",
        "        # Simplified LBP\n",
        "        from skimage.feature import local_binary_pattern\n",
        "        radius = 2\n",
        "        n_points = 8 * radius\n",
        "        lbp = local_binary_pattern(gray, n_points, radius, method='uniform')\n",
        "\n",
        "        # LBP histogram (reduced bins for speed)\n",
        "        n_bins = 10\n",
        "        hist, _ = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins))\n",
        "        hist = hist.astype(float) / (hist.sum() + 1e-7)\n",
        "        features.extend(hist)\n",
        "\n",
        "        # === EDGE FEATURES ===\n",
        "        edges = cv2.Canny(gray, 100, 200)\n",
        "        features.append(np.sum(edges > 0) / edges.size)\n",
        "\n",
        "        # Gradient magnitude\n",
        "        sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
        "        sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
        "        gradient_mag = np.sqrt(sobelx**2 + sobely**2)\n",
        "        features.extend([\n",
        "            np.mean(gradient_mag),\n",
        "            np.std(gradient_mag),\n",
        "            np.max(gradient_mag),\n",
        "        ])\n",
        "\n",
        "        # === COLOUR CORRELATIONS ===\n",
        "        b, g, r = cv2.split(image)\n",
        "        features.append(np.corrcoef(b.flatten(), g.flatten())[0, 1])\n",
        "        features.append(np.corrcoef(g.flatten(), r.flatten())[0, 1])\n",
        "        features.append(np.corrcoef(r.flatten(), b.flatten())[0, 1])\n",
        "\n",
        "        return np.array(features, dtype=np.float32)\n",
        "\n",
        "    def extract_from_batch_vectorized(self, images):\n",
        "        batch_features = []\n",
        "        for img in images:\n",
        "            features = self.extract_features(img)\n",
        "            batch_features.append(features)\n",
        "        return np.array(batch_features, dtype=np.float32)\n",
        "\n",
        "def load_and_extract_folder_fast(folder_path, label, extractor,\n",
        "                                 img_size=(256, 256), chunk_size=5000):\n",
        "    if not os.path.exists(folder_path):\n",
        "        raise FileNotFoundError(f\"Folder not found: {folder_path}\")\n",
        "\n",
        "    image_files = [f for f in os.listdir(folder_path)\n",
        "                  if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
        "\n",
        "    total_images = len(image_files)\n",
        "    print(f\"Processing {total_images} images from {os.path.basename(folder_path)}...\")\n",
        "\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Process in large chunks\n",
        "    for i in tqdm(range(0, total_images, chunk_size)):\n",
        "        chunk_files = image_files[i:i+chunk_size]\n",
        "        chunk_images = []\n",
        "\n",
        "        # Load chunk\n",
        "        for img_file in chunk_files:\n",
        "            img_path = os.path.join(folder_path, img_file)\n",
        "            try:\n",
        "                img = cv2.imread(img_path)\n",
        "                if img is not None:\n",
        "                    img = cv2.resize(img, img_size)\n",
        "                    chunk_images.append(img)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        if chunk_images:\n",
        "            # Extract features from chunk\n",
        "            chunk_images = np.array(chunk_images)\n",
        "            chunk_features = extractor.extract_from_batch_vectorized(chunk_images)\n",
        "            chunk_labels = np.full(len(chunk_features), label)\n",
        "\n",
        "            all_features.append(chunk_features)\n",
        "            all_labels.append(chunk_labels)\n",
        "\n",
        "            # Clear memory\n",
        "            del chunk_images, chunk_features, chunk_labels\n",
        "            gc.collect()\n",
        "\n",
        "    # Combine all chunks\n",
        "    features = np.vstack(all_features)\n",
        "    labels = np.concatenate(all_labels)\n",
        "\n",
        "    del all_features, all_labels\n",
        "    gc.collect()\n",
        "\n",
        "    return features, labels\n",
        "\n",
        "\n",
        "def load_split_fast(dataset_path, split_name, extractor, img_size=(256, 256)):\n",
        "    split_path = os.path.join(dataset_path, 'Dataset', split_name)\n",
        "    real_path = os.path.join(split_path, 'Real')\n",
        "    fake_path = os.path.join(split_path, 'Fake')\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Processing {split_name} Set\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    if not os.path.exists(real_path):\n",
        "        raise FileNotFoundError(f\"Real folder not found: {real_path}\")\n",
        "    if not os.path.exists(fake_path):\n",
        "        raise FileNotFoundError(f\"Fake folder not found: {fake_path}\")\n",
        "\n",
        "    # Process Real images\n",
        "    start = time.time()\n",
        "    print(\"Processing Real images...\")\n",
        "    X_real, y_real = load_and_extract_folder_fast(real_path, 0, extractor, img_size)\n",
        "    real_time = time.time() - start\n",
        "    print(f\"  Time: {real_time:.2f}s ({len(X_real)/real_time:.0f} images/sec)\")\n",
        "\n",
        "    # Process Fake images\n",
        "    start = time.time()\n",
        "    print(\"Processing Fake images...\")\n",
        "    X_fake, y_fake = load_and_extract_folder_fast(fake_path, 1, extractor, img_size)\n",
        "    fake_time = time.time() - start\n",
        "    print(f\"  Time: {fake_time:.2f}s ({len(X_fake)/fake_time:.0f} images/sec)\")\n",
        "\n",
        "    # Combine\n",
        "    X = np.vstack([X_real, X_fake])\n",
        "    y = np.concatenate([y_real, y_fake])\n",
        "\n",
        "    del X_real, X_fake, y_real, y_fake\n",
        "    gc.collect()\n",
        "\n",
        "    print(f\"\\n{split_name} Summary:\")\n",
        "    print(f\"  Total: {len(X)} samples | Features: {X.shape[1]}\")\n",
        "    print(f\"  Real: {np.sum(y==0)} | Fake: {np.sum(y==1)}\")\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\n",
        "# ========== MAIN PROCESSING ==========\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ULTRA-FAST FEATURE EXTRACTION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Initialize fast extractor\n",
        "extractor = FastFeatureExtractor()\n",
        "\n",
        "# Test on a few images first\n",
        "print(\"\\nTesting extractor speed...\")\n",
        "test_img = np.random.randint(0, 255, (256, 256, 3), dtype=np.uint8)\n",
        "start = time.time()\n",
        "for _ in range(100):\n",
        "    _ = extractor.extract_features(test_img)\n",
        "test_time = time.time() - start\n",
        "print(f\"Speed test: {100/test_time:.0f} images/second per core\")\n",
        "\n",
        "# Process all splits\n",
        "splits_to_try = {\n",
        "    'train': ['Train', 'train'],\n",
        "    'val': ['Validation', 'validation', 'Val', 'val'],\n",
        "    'test': ['Test', 'test']\n",
        "}\n",
        "\n",
        "def load_split_safe(base_path, split_variations, extractor):\n",
        "    for variation in split_variations:\n",
        "        try:\n",
        "            return load_split_fast(base_path, variation, extractor)\n",
        "        except FileNotFoundError:\n",
        "            continue\n",
        "    raise FileNotFoundError(f\"Could not find split\")\n",
        "\n",
        "# Process each split with timing\n",
        "total_start = time.time()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PROCESSING TRAINING SET\")\n",
        "print(\"=\"*70)\n",
        "X_train, y_train = load_split_safe(dataset_path, splits_to_try['train'], extractor)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PROCESSING VALIDATION SET\")\n",
        "print(\"=\"*70)\n",
        "X_val, y_val = load_split_safe(dataset_path, splits_to_try['val'], extractor)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PROCESSING TEST SET\")\n",
        "print(\"=\"*70)\n",
        "X_test, y_test = load_split_safe(dataset_path, splits_to_try['test'], extractor)\n",
        "\n",
        "total_time = time.time() - total_start\n",
        "total_samples = len(X_train) + len(X_val) + len(X_test)\n",
        "\n",
        "print(f\"\\n⏱️  Total extraction time: {total_time/60:.2f} minutes\")\n",
        "print(f\"⚡ Speed: {total_samples/total_time:.0f} images/second\")\n",
        "\n",
        "# ========== FEATURE ENGINEERING ==========\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FEATURE ENGINEERING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# Normalize\n",
        "print(\"Normalizing features...\")\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Normalized\")\n",
        "\n",
        "# Feature selection\n",
        "if USE_FEATURE_SELECTION and N_FEATURES_TO_SELECT < X_train_scaled.shape[1]:\n",
        "    print(f\"\\nSelecting top {N_FEATURES_TO_SELECT} features...\")\n",
        "    selector = SelectKBest(score_func=f_classif, k=N_FEATURES_TO_SELECT)\n",
        "    X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
        "    X_val_selected = selector.transform(X_val_scaled)\n",
        "    X_test_selected = selector.transform(X_test_scaled)\n",
        "    print(f\"Selected: {X_train_scaled.shape[1]} → {X_train_selected.shape[1]}\")\n",
        "else:\n",
        "    X_train_selected = X_train_scaled\n",
        "    X_val_selected = X_val_scaled\n",
        "    X_test_selected = X_test_scaled\n",
        "\n",
        "# Clean up\n",
        "del X_train, X_val, X_test, X_train_scaled, X_val_scaled, X_test_scaled\n",
        "gc.collect()\n",
        "\n",
        "# ========== SUMMARY ==========\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"READY FOR TRAINING\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nTotal samples: {total_samples}\")\n",
        "print(f\"Features per sample: {X_train_selected.shape[1]}\")\n",
        "print(f\"Extraction time: {total_time/60:.2f} minutes\")\n",
        "print(f\"Processing speed: {total_samples/total_time:.0f} images/sec\")\n",
        "\n",
        "print(f\"\\nData splits:\")\n",
        "print(f\"  Train: {len(X_train_selected):>6} ({len(X_train_selected)/total_samples*100:.1f}%)\")\n",
        "print(f\"  Val:   {len(X_val_selected):>6} ({len(X_val_selected)/total_samples*100:.1f}%)\")\n",
        "print(f\"  Test:  {len(X_test_selected):>6} ({len(X_test_selected)/total_samples*100:.1f}%)\")\n",
        "\n",
        "print(\"\\nFeature extraction complete - ready for model training!\")\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0MNRXA_Eobk",
        "outputId": "d768138d-9c0d-472f-9dc4-53e2d4f8b162"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "ULTRA-FAST FEATURE EXTRACTION\n",
            "======================================================================\n",
            "\n",
            "Testing extractor speed...\n",
            "Speed test: 20 images/second per core\n",
            "\n",
            "======================================================================\n",
            "PROCESSING TRAINING SET\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Processing Train Set\n",
            "======================================================================\n",
            "Processing Real images...\n",
            "Processing 70001 images from Real...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [54:42<00:00, 218.85s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Time: 3283.45s (21 images/sec)\n",
            "Processing Fake images...\n",
            "Processing 70001 images from Fake...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [55:39<00:00, 222.61s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Time: 3339.59s (21 images/sec)\n",
            "\n",
            "Train Summary:\n",
            "  Total: 140002 samples | Features: 47\n",
            "  Real: 70001 | Fake: 70001\n",
            "\n",
            "======================================================================\n",
            "PROCESSING VALIDATION SET\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Processing Validation Set\n",
            "======================================================================\n",
            "Processing Real images...\n",
            "Processing 19787 images from Real...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [15:57<00:00, 239.45s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Time: 958.24s (21 images/sec)\n",
            "Processing Fake images...\n",
            "Processing 19641 images from Fake...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [15:52<00:00, 238.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Time: 952.60s (21 images/sec)\n",
            "\n",
            "Validation Summary:\n",
            "  Total: 39428 samples | Features: 47\n",
            "  Real: 19787 | Fake: 19641\n",
            "\n",
            "======================================================================\n",
            "PROCESSING TEST SET\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Processing Test Set\n",
            "======================================================================\n",
            "Processing Real images...\n",
            "Processing 5413 images from Real...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [04:18<00:00, 129.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Time: 258.22s (21 images/sec)\n",
            "Processing Fake images...\n",
            "Processing 5492 images from Fake...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [04:25<00:00, 132.80s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Time: 265.80s (21 images/sec)\n",
            "\n",
            "Test Summary:\n",
            "  Total: 10905 samples | Features: 47\n",
            "  Real: 5413 | Fake: 5492\n",
            "\n",
            "⏱️  Total extraction time: 150.97 minutes\n",
            "⚡ Speed: 21 images/second\n",
            "\n",
            "======================================================================\n",
            "FEATURE ENGINEERING\n",
            "======================================================================\n",
            "Normalizing features...\n",
            "Normalized\n",
            "\n",
            "======================================================================\n",
            "READY FOR TRAINING\n",
            "======================================================================\n",
            "\n",
            "Total samples: 190335\n",
            "Features per sample: 47\n",
            "Extraction time: 150.97 minutes\n",
            "Processing speed: 21 images/sec\n",
            "\n",
            "Data splits:\n",
            "  Train: 140002 (73.6%)\n",
            "  Val:    39428 (20.7%)\n",
            "  Test:   10905 (5.7%)\n",
            "\n",
            "Feature extraction complete - ready for model training!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING IMPROVED RANDOM FOREST\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Better hyperparameters for deepfake detection\n",
        "model = RandomForestClassifier(\n",
        "    n_estimators=500,          # More trees = more stable\n",
        "    max_depth=12,              # SHALLOWER trees (was 20)\n",
        "    min_samples_split=20,      # Need MORE samples to split (was 2)\n",
        "    min_samples_leaf=10,       # LARGER leaves (was 1)\n",
        "    max_features='sqrt',       # Use sqrt(n) features per tree\n",
        "    max_samples=0.7,           # Use only 70% of data per tree (bootstrap)\n",
        "    class_weight='balanced',   # Handle class imbalance\n",
        "    bootstrap=True,\n",
        "    oob_score=True,            # Out-of-bag score for validation\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(f\"Model configuration:\")\n",
        "print(f\"  Trees: {model.n_estimators}\")\n",
        "print(f\"  Max depth: {model.max_depth}\")\n",
        "print(f\"  Class weight: balanced\")\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "model.fit(X_train_selected, y_train)\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nTraining complete in {training_time:.2f} seconds\")\n",
        "\n",
        "# Training accuracy\n",
        "train_pred = model.predict(X_train_selected)\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "print(f\"Training Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "in6vzBuan_AW",
        "outputId": "bc01fc6e-3bfb-4503-a14e-e51b55c56eb0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "TRAINING IMPROVED RANDOM FOREST\n",
            "======================================================================\n",
            "Model configuration:\n",
            "  Trees: 500\n",
            "  Max depth: 12\n",
            "  Class weight: balanced\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   32.9s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  2.3min\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  5.3min\n",
            "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  5.9min finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training complete in 366.67 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.5s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    2.2s\n",
            "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:    6.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.7527 (75.27%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=2)]: Done 500 out of 500 | elapsed:    7.0s finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X, y, dataset_name=\"Test\"):\n",
        "    \"\"\"Evaluate model and print metrics\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"{dataset_name} Set Evaluation\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = model.predict(X)\n",
        "    y_pred_proba = model.predict_proba(X)[:, 1]\n",
        "\n",
        "    # Metrics\n",
        "    accuracy = accuracy_score(y, y_pred)\n",
        "    precision = precision_score(y, y_pred)\n",
        "    recall = recall_score(y, y_pred)\n",
        "    f1 = f1_score(y, y_pred)\n",
        "    auc = roc_auc_score(y, y_pred_proba)\n",
        "\n",
        "    print(f\"\\nMetrics:\")\n",
        "    print(f\"  Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall:    {recall:.4f}\")\n",
        "    print(f\"  F1-Score:  {f1:.4f}\")\n",
        "    print(f\"  ROC-AUC:   {auc:.4f}\")\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y, y_pred)\n",
        "    print(f\"\\nConfusion Matrix:\")\n",
        "    print(f\"  TN (Real→Real): {cm[0,0]:>5}  |  FP (Real→Fake): {cm[0,1]:>5}\")\n",
        "    print(f\"  FN (Fake→Real): {cm[1,0]:>5}  |  TP (Fake→Fake): {cm[1,1]:>5}\")\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'auc': auc,\n",
        "        'confusion_matrix': cm,\n",
        "        'y_pred': y_pred,\n",
        "        'y_pred_proba': y_pred_proba\n",
        "    }\n",
        "\n",
        "# Evaluate on validation and test sets\n",
        "print(\"\\n=== Model Evaluation ===\")\n",
        "val_metrics = evaluate_model(model, X_val_selected, y_val, \"Validation\")\n",
        "test_metrics = evaluate_model(model, X_test_selected, y_test, \"Test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WrfQPXAo_vl",
        "outputId": "306955db-eede-4dce-b9ab-41669b9091d0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Model Evaluation ===\n",
            "\n",
            "============================================================\n",
            "Validation Set Evaluation\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.7s\n",
            "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:    1.5s\n",
            "[Parallel(n_jobs=2)]: Done 500 out of 500 | elapsed:    1.7s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.7s\n",
            "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:    1.6s\n",
            "[Parallel(n_jobs=2)]: Done 500 out of 500 | elapsed:    1.7s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metrics:\n",
            "  Accuracy:  0.7091 (70.91%)\n",
            "  Precision: 0.6711\n",
            "  Recall:    0.8161\n",
            "  F1-Score:  0.7365\n",
            "  ROC-AUC:   0.7905\n",
            "\n",
            "Confusion Matrix:\n",
            "  TN (Real→Real): 11930  |  FP (Real→Fake):  7857\n",
            "  FN (Fake→Real):  3612  |  TP (Fake→Fake): 16029\n",
            "\n",
            "============================================================\n",
            "Test Set Evaluation\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:    0.5s\n",
            "[Parallel(n_jobs=2)]: Done 500 out of 500 | elapsed:    0.6s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metrics:\n",
            "  Accuracy:  0.5644 (56.44%)\n",
            "  Precision: 0.5475\n",
            "  Recall:    0.7788\n",
            "  F1-Score:  0.6430\n",
            "  ROC-AUC:   0.5837\n",
            "\n",
            "Confusion Matrix:\n",
            "  TN (Real→Real):  1878  |  FP (Real→Fake):  3535\n",
            "  FN (Fake→Real):  1215  |  TP (Fake→Fake):  4277\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:    0.5s\n",
            "[Parallel(n_jobs=2)]: Done 500 out of 500 | elapsed:    0.5s finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "id": "9r9Ggw012g9c"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVmFDcYOSNiV"
      },
      "source": [
        "# Gemini API: Getting started with Gemini models\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=30/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6DgLLlGOFfx"
      },
      "source": [
        "---\n",
        "> **Gemini 3 Pro/Flash**: If you are only interested in the new [Gemini 3 models](https://ai.google.dev/gemini-api/docs/gemini-3) new capabilities ([thinking levels](#thinking_level), [media resolution](#media_resolution) and [thoughts signatures](#thoughts_signature), jump directly to the [dedicated section](#gemini3) at the end of this notebook.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrWvVAIP3c1v"
      },
      "source": [
        "The **[Google Gen AI SDK](https://github.com/googleapis/python-genai)** provides a unified interface to [Gemini models](https://ai.google.dev/gemini-api/docs/models) through both the [Gemini Developer API](https://ai.google.dev/gemini-api/docs) and the Gemini API on [Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/overview). With a few exceptions, code that runs on one platform will run on both. This notebook uses the Developer API.\n",
        "\n",
        "This notebook will walk you through:\n",
        "\n",
        "* [Installing and setting-up](#setup) the Google GenAI SDK\n",
        "* [Text](#text_prompt) and [multimodal](#multimodal_prompt) prompting\n",
        "* Setting [system instructions](#system_instructions)\n",
        "* Control the [thinking](#thinking) process\n",
        "* Counting [tokens](#count_tokens)\n",
        "* Configuring [safety filters](#safety_filters)\n",
        "* Initiating a [multi-turn chat](#chat)\n",
        "* Generating a [content stream](#stream) and sending [asynchronous](#async) requests\n",
        "* [Controlling generated output](#json)\n",
        "* Using [function calling](#function_calling)\n",
        "* Grounding your requests using [file uploads](#file_api), [Google Search](#search_grounding), [Google Maps](#maps), [Youtube](#youtube_link) or by add [URLs](#url_context) to you prompt\n",
        "* Using [context caching](#caching)\n",
        "* Generating [text embeddings](#embeddings)\n",
        "\n",
        "More details about the SDK on the [documentation](https://ai.google.dev/gemini-api/docs/sdks)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xlNFlmEeXmC"
      },
      "source": [
        "Feature-specific models have their own dedicated guides:\n",
        "* Podcast and speech generation using [Gemini TTS ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](./Get_started_TTS.ipynb),\n",
        "* Live interaction with [Gemini Live ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](./Get_started_LiveAPI.ipynb),\n",
        "* Image generation using [Imagen ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](./Get_started_imagen.ipynb),\n",
        "* Video generation using [Veo ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](./Get_started_Veo.ipynb),\n",
        "* Music generation using [Lyria RealTime ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](./Get_started_LyriaRealTime.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfk6YY3G5kqp"
      },
      "source": [
        "<a name=\"setup\"></a>\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5027929de8f"
      },
      "source": [
        "### Install SDK\n",
        "\n",
        "Install the SDK from [PyPI](https://github.com/googleapis/python-genai). It's recommended to always use the latest version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "46zEFO2a9FFd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdf4a371-ec35-40d9-f4d7-34e5aaa95155"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.3/53.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m713.3/713.3 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m234.9/234.9 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires google-auth==2.43.0, but you have google-auth 2.47.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install -U -q 'google-genai>=1.51.0' # 1.51 is needed for Gemini 3 pro thinking levels support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTIfnvCn9HvH"
      },
      "source": [
        "### Setup your API key\n",
        "\n",
        "To run the following cell, your API key must be stored it in a Colab Secret named `GEMINI_API_KEY`. If you don't already have an API key or you aren't sure how to create a Colab Secret, see [Authentication ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](../quickstarts/Authentication.ipynb) for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "A1pkoyZb9Jm3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "d354f71d-c03b-46ae-8195-379e5b790841"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SecretNotFoundError",
          "evalue": "Secret GEMINI_API_KEY does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3261757588.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mGEMINI_API_KEY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GEMINI_API_KEY'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSecretNotFoundError\u001b[0m: Secret GEMINI_API_KEY does not exist."
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Hx_Gw9i0Yuv"
      },
      "source": [
        "### Initialize SDK client\n",
        "\n",
        "With the new SDK, now you only need to initialize a client with you API key (or OAuth if using [Vertex AI](https://cloud.google.com/vertex-ai)). The model is now set in each call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HghvVpbU0Uap"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client(api_key=GEMINI_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvA_mbi1JxD5"
      },
      "source": [
        "### Choose a model\n",
        "\n",
        "Select the model you want to use in this guide. You can either select one from the list or enter a model name manually. Keep in mind that some models, such as the 2.5 ones are thinking models and thus take slightly more time to respond. For more details, you can see [thinking notebook ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](./Get_started_thinking.ipynb) to learn how to control the thinking.\n",
        "\n",
        "Feel free to select [Gemini 3 Pro](https://ai.google.dev/gemini-api/docs/models#gemini-3-pro) if you want to try our newest model, but keep in mind that it has no free tier.\n",
        "\n",
        "For a full overview of all Gemini models, check the [documentation](https://ai.google.dev/gemini-api/docs/models/gemini)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AChpZWIXu62m"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-2.5-flash\" # @param [\"gemini-2.5-flash-lite\", \"gemini-2.5-flash\", \"gemini-2.5-pro\", \"gemini-3-flash-preview\", \"gemini-3-pro-preview\"] {\"allow-input\":true, isTemplate: true}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TYNPrNvQ8ue"
      },
      "source": [
        "<a name=\"text_prompt\"></a>\n",
        "## Send text prompts\n",
        "\n",
        "Use the `generate_content` method to generate responses to your prompts. You can pass text directly to `generate_content` and use the `.text` property to get the text content of the response. Note that the `.text` field will work when there's only one part in the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8md0ayAJ-RZ"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What's the largest planet in our solar system?\"\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEkps79uFZ2P"
      },
      "source": [
        "<a name=\"system_instructions\"></a>\n",
        "## Add system instructions\n",
        "\n",
        "You can also add system instructions to give the model direction on how to respond and which persona it should use. This is especially useful for mixture-of-experts models like the the pro models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMyiCYT3HEqv"
      },
      "outputs": [],
      "source": [
        "system_instruction = \"You are a pirate and are explaining things to 5 years old kids.\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What's the largest planet in our solar system?\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "    )\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9B8pb7tv_Cx"
      },
      "source": [
        "<a name=\"count_tokens\"></a>\n",
        "## Count tokens\n",
        "\n",
        "Tokens are the basic inputs to the Gemini models. You can use the `count_tokens` method to calculate the number of input tokens before sending a request to the Gemini API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7WFm928wEYR"
      },
      "outputs": [],
      "source": [
        "response = client.models.count_tokens(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What's the highest mountain in Africa?\",\n",
        ")\n",
        "\n",
        "print(f\"This prompt was worth {response.total_tokens} tokens.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTzuBfHyWAg5"
      },
      "source": [
        "<a name=\"parameters\"></a>\n",
        "## Configure model parameters\n",
        "\n",
        "You can include parameter values in each call that you send to a model to control how the model generates a response.\n",
        "\n",
        "Learn more about [experimenting with parameter values](https://ai.google.dev/gemini-api/docs/prompting-strategies#model-parameters) in the documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5izy6jsbEnL"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        temperature=0.4, # Temperature of 1 is strongly recommended for Gemini 3 Pro\n",
        "        top_p=0.95,\n",
        "        top_k=20,\n",
        "        candidate_count=1,\n",
        "        seed=5,\n",
        "        stop_sequences=[\"STOP!\"],\n",
        "        presence_penalty=0.0,\n",
        "        frequency_penalty=0.0,\n",
        "    )\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3gCda1c0Dza"
      },
      "source": [
        "<a name=\"thinking\"></a>\n",
        "## Control the thinking process\n",
        "\n",
        "All models since the 2.5 generation are thinking models, which means that they are first analysing your request, strategizing about how to answer and only afterwards starting to answer you. This is very useful for complex requests but at the cost of some latency.\n",
        "\n",
        "Check the [dedicated guide ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](./Get_started_thinking.ipynb) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyCUGbkpN0JP"
      },
      "source": [
        "### Check the thought process\n",
        "\n",
        "By adding the `include_thoughts=True` option in the config, you can check the though proces of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgKqAaePSpPJ"
      },
      "outputs": [],
      "source": [
        "prompt = \"A man moves his car to an hotel and tells the owner he’s bankrupt. Why?\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "  model=MODEL_ID,\n",
        "  contents=prompt,\n",
        "  config=types.GenerateContentConfig(\n",
        "    thinking_config=types.ThinkingConfig(\n",
        "      include_thoughts=True\n",
        "    )\n",
        "  )\n",
        ")\n",
        "\n",
        "for part in response.parts:\n",
        "  if not part.text:\n",
        "    continue\n",
        "  if part.thought:\n",
        "    display(Markdown(\"### Thought summary:\"))\n",
        "    display(Markdown(part.text))\n",
        "    print()\n",
        "  else:\n",
        "    display(Markdown(\"### Answer:\"))\n",
        "    display(Markdown(part.text))\n",
        "    print()\n",
        "\n",
        "print(f\"We used {response.usage_metadata.thoughts_token_count} tokens for the thinking phase and {response.usage_metadata.prompt_token_count} for the output.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kn3sO4T6CGms"
      },
      "source": [
        "### Disable thinking\n",
        "\n",
        "On flash and flash-lite models, you can disable the thinking by setting its `thinking_budget` to 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLpaiKKlCkqL"
      },
      "outputs": [],
      "source": [
        "if \"-pro\" not in MODEL_ID:\n",
        "  response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Quicky tell me a joke about unicorns.\",\n",
        "    config=types.GenerateContentConfig(\n",
        "      thinking_config=types.ThinkingConfig(\n",
        "        thinking_budget=0\n",
        "      )\n",
        "    )\n",
        "  )\n",
        "\n",
        "  display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_Qp4zszCygZ"
      },
      "source": [
        "Inversely, you can also use `thinking_budget` to set it even higher (up to 24576 tokens).\n",
        "\n",
        "For Gemini 3, please check the [dedicated section](#thinking_level) at the end of this guide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yww-vrxmRiIy"
      },
      "source": [
        "<a name=\"multimodal_prompt\"></a>\n",
        "## Send multimodal prompts\n",
        "\n",
        "Use Gemini model, a multimodal model that supports multimodal prompts. You can include text, [PDF documents ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](../quickstarts/PDF_Files.ipynb), images, [audio ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](../quickstarts/Audio.ipynb) and [videos ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](../quickstarts/Video.ipynb) in your prompt requests and get text or code responses. Check the [File API](#file_api) section below for more examples.\n",
        "\n",
        "In this first example, you'll download an image from a specified URL, save it as a byte stream and then write those bytes to a local file named `jetpack.png`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQ3zu5udSBuD"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pathlib\n",
        "from PIL import Image\n",
        "\n",
        "IMG = \"https://storage.googleapis.com/generativeai-downloads/data/jetpack.png\" # @param {type: \"string\"}\n",
        "\n",
        "img_bytes = requests.get(IMG).content\n",
        "\n",
        "img_path = pathlib.Path('jetpack.png')\n",
        "img_path.write_bytes(img_bytes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSjAMbVjOlnc"
      },
      "source": [
        "Now send the image, and ask Gemini to generate a short blog post based on it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDxd7Pp_SELb"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "image = Image.open(img_path)\n",
        "image.thumbnail([512,512])\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        image,\n",
        "        \"Write a short and engaging blog post based on this picture.\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "display(image)\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHLitKXj1hZa"
      },
      "source": [
        "<a name=\"images\"></a>\n",
        "## Generate Images\n",
        "\n",
        "Gemini can output images directly as part of a conversation using the [Image generation ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](./Image_out.ipynb) models (aka \"Nano-banana)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbdnNzGL6R_2"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, Markdown\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash-image\",\n",
        "    contents='Hi, can you create a 3d rendered image of a pig with wings and a top hat flying over a happy futuristic scifi city with lots of greenery?',\n",
        "    config=types.GenerateContentConfig(\n",
        "        response_modalities=['Text', 'Image']\n",
        "    )\n",
        ")\n",
        "\n",
        "for part in response.parts:\n",
        "  if part.text is not None:\n",
        "    display(Markdown(part.text))\n",
        "  elif part.inline_data is not None:\n",
        "    generated_image = part.as_image()\n",
        "    generated_image.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTAnYx_bbxPk"
      },
      "source": [
        "<a name=\"safety_filters\"></a>\n",
        "## Configure safety filters\n",
        "\n",
        "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what is appropriate for your use case. See the [Configure safety filters](https://ai.google.dev/gemini-api/docs/safety-settings) documentation for details.\n",
        "\n",
        "\n",
        "In this example, you'll use a safety filter to only block highly dangerous content, when requesting the generation of potentially disrespectful phrases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJIvAfMqbzQL"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "    Write a list of 2 disrespectful things that I might say to the universe after stubbing my toe in the dark.\n",
        "\"\"\"\n",
        "\n",
        "safety_settings = [\n",
        "    types.SafetySetting(\n",
        "        category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "        threshold=\"BLOCK_ONLY_HIGH\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "        safety_settings=safety_settings,\n",
        "    ),\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6sB7W-jdGxJ"
      },
      "source": [
        "<a name=\"chat\"></a>\n",
        "## Start a multi-turn chat\n",
        "\n",
        "The Gemini API enables you to have freeform conversations across multiple turns.\n",
        "\n",
        "Next you'll set up a helpful coding assistant:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIC0pLnJdI8m"
      },
      "outputs": [],
      "source": [
        "system_instruction = \"\"\"\n",
        "  You are an expert software developer and a helpful coding assistant.\n",
        "  You are able to generate high-quality code in any programming language.\n",
        "\"\"\"\n",
        "\n",
        "chat_config = types.GenerateContentConfig(\n",
        "    system_instruction=system_instruction,\n",
        ")\n",
        "\n",
        "chat = client.chats.create(\n",
        "    model=MODEL_ID,\n",
        "    config=chat_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVQZitGE7EbW"
      },
      "source": [
        "Use `chat.send_message` to pass a message back and receive a response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnzMJJ-adOfX"
      },
      "outputs": [],
      "source": [
        "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWWcDneQO4Ya"
      },
      "source": [
        "Here's another example using your new helpful coding assistant:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qAObeZMdgln"
      },
      "outputs": [],
      "source": [
        "response = chat.send_message(\"Okay, write a unit test of the generated function.\")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNX4AQ9AWddm"
      },
      "source": [
        "### Save and resume a chat\n",
        "\n",
        "Most objects in the Python SDK are implemented as [Pydantic models](https://docs.pydantic.dev/latest/concepts/models/). As Pydantic has a number of features for serializing and deserializing objects, you can use them for persistence.\n",
        "\n",
        "This example shows how to save and restore a [`Chat`](https://googleapis.github.io/python-genai/genai.html#genai.chats.Chat) session using JSON."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuoSfLW5Xl1V"
      },
      "outputs": [],
      "source": [
        "from pydantic import TypeAdapter\n",
        "\n",
        "# Chat history is a list of Content objects. A TypeAdapter can convert to and from\n",
        "# these Pydantic types.\n",
        "history_adapter = TypeAdapter(list[types.Content])\n",
        "\n",
        "# Use the chat object from the previous section.\n",
        "chat_history = chat.get_history()\n",
        "\n",
        "# Convert to a JSON list.\n",
        "json_history = history_adapter.dump_json(chat_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4xU6wpPZRdL"
      },
      "source": [
        "At this point you can save the JSON bytestring to disk or wherever you persist data. When you load it again, you can instantiate a new chat session using the stored history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hq1bAsX8ZRS3"
      },
      "outputs": [],
      "source": [
        "# Convert the JSON back to the Pydantic schema.\n",
        "history = history_adapter.validate_json(json_history)\n",
        "\n",
        "# Now load a new chat session using the JSON history.\n",
        "new_chat = client.chats.create(\n",
        "    model=MODEL_ID,\n",
        "    config=chat_config,\n",
        "    history=history,\n",
        ")\n",
        "\n",
        "response = new_chat.send_message(\"What was the name of the function again?\")\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyZMoM6tgnTA"
      },
      "source": [
        "<a name=\"json\"></a>\n",
        "## Generate JSON\n",
        "\n",
        "The [controlled generation](https://ai.google.dev/gemini-api/docs/structured-output?lang=python#generate-json) (aka. \"Structured output\") capability in Gemini API allows you to constraint the model output to a structured format. You can provide the schemas as Pydantic Models or a JSON string."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w1PA0zTgXa7"
      },
      "source": [
        "You can find more examples of controlled generation in the [dedicated notebook ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](./JSON.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRJHVjr-gqHi"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel\n",
        "import json\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "    recipe_name: str\n",
        "    recipe_description: str\n",
        "    recipe_ingredients: list[str]\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Provide a popular cookie recipe and its ingredients.\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=Recipe,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(json.dumps(json.loads(response.text), indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1tAq1kSxgoY"
      },
      "source": [
        "[Image ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](./Get_started_imagen.ipynb) is another way to generate images. See the [documentation](https://ai.google.dev/gemini-api/docs/image-generation#choose-a-model) for recommendations on where to use each one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQfLCxfQtPTg"
      },
      "source": [
        "<a name=\"stream\"></a>\n",
        "## Generate content stream\n",
        "\n",
        "By default, the model returns a response after completing the entire generation process. You can also use the `generate_content_stream` method to stream the response as it's being generated, and the model will return chunks of the response as soon as they're generated.\n",
        "\n",
        "Note that if you're using a thinking model, it'll only start streaming after finishing its thinking process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gIsSNqXtOXB"
      },
      "outputs": [],
      "source": [
        "for chunk in client.models.generate_content_stream(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Tell me a story about a lonely robot who finds friendship in a most unexpected place.\"\n",
        "):\n",
        "  print(chunk.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plCtEIaHuv96"
      },
      "source": [
        "<a name=\"async\"></a>\n",
        "## Send asynchronous requests\n",
        "\n",
        "`client.aio` exposes all the analogous async methods that are available on `client`.\n",
        "\n",
        "For example, `client.aio.models.generate_content` is the async version of `client.models.generate_content`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSNoANFuhBby"
      },
      "source": [
        "More details in the [dedicated guide ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](./asynchronous_requests.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPTI7noYuwgr"
      },
      "outputs": [],
      "source": [
        "response = await client.aio.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Compose a song about the adventures of a time-traveling squirrel.\"\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enBhuaIk3KYa"
      },
      "source": [
        "<a name=\"file_api\"></a>\n",
        "## Upload files\n",
        "\n",
        "Now that you've seen how to send multimodal prompts, try uploading files to the API of different multimedia types. For small images, such as the previous multimodal example, you can point the Gemini model directly to a local file when providing a prompt. When you've larger files, many files, or files you don't want to send over and over again, you can use the [File Upload API](https://ai.google.dev/gemini-api/docs/files), and then pass the file by reference.\n",
        "\n",
        "More examples and details in the [File API guide ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](./File-API.ipynb), or the guides dedicated to [Audio![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](./Audio.ipynb), [Video![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](./Video_understanding.ipynb) or [Image/Spatial![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](./Spatial_understanding.ipynb) understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1y5eZmqThDR"
      },
      "source": [
        "### Upload a large text file\n",
        "\n",
        "Let's start by uploading a text file. In this case, you'll use a 400 page transcript from [Apollo 11](https://www.nasa.gov/history/alsj/a11/a11trans.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Sa6lEH9Tjm5"
      },
      "outputs": [],
      "source": [
        "# Prepare the file to be uploaded\n",
        "TEXT = \"https://storage.googleapis.com/generativeai-downloads/data/a11.txt\"  # @param {type: \"string\"}\n",
        "text_bytes = requests.get(TEXT).content\n",
        "\n",
        "text_path = pathlib.Path('a11.txt')\n",
        "text_path.write_bytes(text_bytes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kC3bJQJcUKFk"
      },
      "outputs": [],
      "source": [
        "# Upload the file using the API\n",
        "file_upload = client.files.upload(file=text_path)\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        file_upload,\n",
        "        \"Can you give me a summary of this information please?\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGswPFKrTvby"
      },
      "source": [
        "### Upload an image file\n",
        "\n",
        "You can also upload images so that it's easier to use them multiple time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8FVVJHb828le"
      },
      "outputs": [],
      "source": [
        "# Prepare the file to be uploaded\n",
        "IMG = \"https://storage.googleapis.com/generativeai-downloads/data/jetpack.png\"  # @param {type: \"string\"}\n",
        "img_bytes = requests.get(IMG).content\n",
        "\n",
        "img_path = pathlib.Path('jetpack.png')\n",
        "img_path.write_bytes(img_bytes)\n",
        "\n",
        "media_resolution = 'MEDIA_RESOLUTION_HIGH' # @param ['MEDIA_RESOLUTION_UNSPECIFIED','MEDIA_RESOLUTION_LOW','MEDIA_RESOLUTION_MEDIUM','MEDIA_RESOLUTION_HIGH']\n",
        "# You can also use types.MediaResolution.MEDIA_RESOLUTION_LOW/MEDIUM/HIGH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlJ9NwRGT6d1"
      },
      "outputs": [],
      "source": [
        "# Upload the file using the API\n",
        "file_upload = client.files.upload(file=img_path)\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        file_upload,\n",
        "        \"Write a short and engaging blog post based on this picture.\",\n",
        "    ],\n",
        "    config=types.GenerateContentConfig(\n",
        "        media_resolution=media_resolution\n",
        "    )\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQYxSZtOrgjd"
      },
      "source": [
        "The previous example was also using `media_resolution` to tell the model if it if should\n",
        "\n",
        "You'll find a lot of examples of the image analysis capabilities of the Gemini models in the [Spatial understanding ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](./Spatial_understanding.ipynb) notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLV19RrMUlaw"
      },
      "source": [
        "### Upload a PDF file\n",
        "\n",
        "This PDF page is an article titled [Smoothly editing material properties of objects](https://research.google/blog/smoothly-editing-material-properties-of-objects-with-text-to-image-models-and-synthetic-data/) with text-to-image models and synthetic data available on the Google Research Blog.\n",
        "\n",
        "Firstly you'll download a the PDF file from an URL and save it locally as \"article.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0BfhLDFWfCS"
      },
      "outputs": [],
      "source": [
        "# Prepare the file to be uploaded\n",
        "PDF = \"https://storage.googleapis.com/generativeai-downloads/data/Smoothly%20editing%20material%20properties%20of%20objects%20with%20text-to-image%20models%20and%20synthetic%20data.pdf\"  # @param {type: \"string\"}\n",
        "pdf_bytes = requests.get(PDF).content\n",
        "\n",
        "pdf_path = pathlib.Path('article.pdf')\n",
        "pdf_path.write_bytes(pdf_bytes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjrfdaiYPuIL"
      },
      "source": [
        "Secondly, you'll upload the saved PDF file and generate a bulleted list summary of its contents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tH2h2WDVWptt"
      },
      "outputs": [],
      "source": [
        "# Upload the file using the API\n",
        "file_upload = client.files.upload(file=pdf_path)\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        file_upload,\n",
        "        \"Can you summarize this file as a bulleted list?\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NWO1moe9fx-"
      },
      "source": [
        "### Upload an audio file\n",
        "\n",
        "In this case, you'll use a [sound recording](https://www.jfklibrary.org/asset-viewer/archives/jfkwha-006) of President John F. Kennedy’s 1961 State of the Union address."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCSuGd9i9fEB"
      },
      "outputs": [],
      "source": [
        "# Prepare the file to be uploaded\n",
        "AUDIO = \"https://storage.googleapis.com/generativeai-downloads/data/State_of_the_Union_Address_30_January_1961.mp3\"  # @param {type: \"string\"}\n",
        "audio_bytes = requests.get(AUDIO).content\n",
        "\n",
        "audio_path = pathlib.Path('audio.mp3')\n",
        "audio_path.write_bytes(audio_bytes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wjKO0eI9yps"
      },
      "outputs": [],
      "source": [
        "# Upload the file using the API\n",
        "file_upload = client.files.upload(file=audio_path)\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        file_upload,\n",
        "        \"Listen carefully to the following audio file. Provide a brief summary\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdUjkIQP-G_i"
      },
      "source": [
        "### Upload a video file\n",
        "\n",
        "In this case, you'll use a short clip of [Big Buck Bunny](https://peach.blender.org/about/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9ohtLxU-SFE"
      },
      "outputs": [],
      "source": [
        "# Download the video file\n",
        "VIDEO_URL = \"https://storage.googleapis.com/generativeai-downloads/videos/Big_Buck_Bunny.mp4\"  # @param {type: \"string\"}\n",
        "video_file_name = \"BigBuckBunny_320x180.mp4\"\n",
        "!wget -O {video_file_name} $VIDEO_URL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyFVXPspS5GF"
      },
      "source": [
        "Let's start by uploading the video file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PY1WlxMk-0Uy"
      },
      "outputs": [],
      "source": [
        "# Upload the file using the API\n",
        "video_file = client.files.upload(file=video_file_name)\n",
        "print(f\"Completed upload: {video_file.uri}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yRG9BPXS65b"
      },
      "source": [
        "> **Note:** The state of the video is important. The video must finish processing, so do check the state. Once the state of the video is `ACTIVE`, you're able to pass it into `generate_content`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEk4P3fK_OcJ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Check the file processing state\n",
        "while video_file.state == \"PROCESSING\":\n",
        "    print('Waiting for video to be processed.')\n",
        "    time.sleep(10)\n",
        "    video_file = client.files.get(name=video_file.name)\n",
        "\n",
        "if video_file.state == \"FAILED\":\n",
        "  raise ValueError(video_file.state)\n",
        "print(f'Video processing complete: ' + video_file.uri)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMz9GIuvAiCO"
      },
      "outputs": [],
      "source": [
        "print(video_file.state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cX82TyGL-e2O"
      },
      "outputs": [],
      "source": [
        "# Ask Gemini about the video\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        video_file,\n",
        "        \"Describe this video.\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlasT2Yur-ye"
      },
      "source": [
        "<a name=\"grounding\"></a>\n",
        "## Grounding\n",
        "\n",
        "The Gemini API give you multiple ways to ground your requests, including Google search, maps, youtube, and url context.\n",
        "\n",
        "For more details information and examples, check the [Grounding ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](./Grounding.ipynb) notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1AFkE5csFYn"
      },
      "source": [
        "<a name=\"search_grounding\"></a>\n",
        "### Ground your requests with Google Search\n",
        "\n",
        "Google Search grounding is particularly useful for queries that require current information or external knowledge.\n",
        "\n",
        "To enable Google Search, simply add the `google_search` tool in the `generate_content`'s `config`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uEIcgEDsvvI"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown, HTML, display\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Who's the current Magic the gathering world champion?\",\n",
        "    config={\"tools\": [{\"google_search\": {}}]},\n",
        ")\n",
        "\n",
        "# print the response\n",
        "display(Markdown(f\"**Response**:\\n {response.text}\"))\n",
        "# print the search details\n",
        "print(f\"Search Query: {response.candidates[0].grounding_metadata.web_search_queries}\")\n",
        "# urls used for grounding\n",
        "print(f\"Search Pages: {', '.join([site.web.title for site in response.candidates[0].grounding_metadata.grounding_chunks])}\")\n",
        "\n",
        "display(HTML(response.candidates[0].grounding_metadata.search_entry_point.rendered_content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tE6vNLcusf2"
      },
      "source": [
        "Note that you should always display the grounding `rendered_content` when using search grounding.\n",
        "\n",
        "Check out the [Search grounding ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](./Search_grounding.ipynb) dedicated guide for more details and examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylPa8XFoYCq_"
      },
      "source": [
        "<a name=\"maps\"></a>\n",
        "### Use Google Maps grounding\n",
        "\n",
        "[Google Maps grounding](https://ai.google.dev/gemini-api/docs/maps-grounding) allows you to easily incorporate location-aware functionality into your applications. When a prompt has context related to Maps data, the Gemini model uses Google Maps to provide factually accurate and fresh answers that are relevant to the specified location or general area.\n",
        "\n",
        "To enable grounding with Google Maps, add the `google_maps` tool in the  `config` argument of `generate_content`, and optionally provide a structured location in the `tool_config`.\n",
        "\n",
        "**Note that Gemini 3 models currently don't support Maps grounding.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AoiEtX9hJRT"
      },
      "outputs": [],
      "source": [
        "if not MODEL_ID.startswith(\"gemini-3\"):\n",
        "  from IPython.display import Markdown\n",
        "\n",
        "  response = client.models.generate_content(\n",
        "      model=MODEL_ID,\n",
        "      contents=\"Do any cafes around here do a good flat white? I will walk up to 20 minutes away\",\n",
        "      config=types.GenerateContentConfig(\n",
        "          tools=[types.Tool(google_maps=types.GoogleMaps())],\n",
        "          tool_config=types.ToolConfig(\n",
        "              retrieval_config=types.RetrievalConfig(\n",
        "                  lat_lng=types.LatLng(latitude=40.7680797, longitude=-73.9818957) # Columbus Circle in New York - https://maps.app.goo.gl/hsQpspc8Vt3AXSrX7\n",
        "              )\n",
        "          ),\n",
        "      ),\n",
        "  )\n",
        "\n",
        "  display(Markdown(f\"### Response\\n {response.text}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dewokmssn2-x"
      },
      "source": [
        "All grounded outputs require sources to be displayed after the response text. This code snippet will display the sources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3mktcrzoMCp"
      },
      "outputs": [],
      "source": [
        "def generate_sources(response: types.GenerateContentResponse):\n",
        "  grounding = response.candidates[0].grounding_metadata\n",
        "  # You only need to display sources that were part of the grounded response.\n",
        "  supported_chunk_indices = {i for support in grounding.grounding_supports for i in support.grounding_chunk_indices}\n",
        "\n",
        "  sources = []\n",
        "  if supported_chunk_indices:\n",
        "    sources.append(\"### Sources from Google Maps\")\n",
        "  for i in supported_chunk_indices:\n",
        "    ref = grounding.grounding_chunks[i].maps\n",
        "    sources.append(f\"- [{ref.title}]({ref.uri})\")\n",
        "\n",
        "  return \"\\n\".join(sources)\n",
        "\n",
        "\n",
        "display(Markdown(generate_sources(response)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x99qIgIuwfnD"
      },
      "source": [
        "More details, inlcuding how to render the contextual Google Maps widget, check the [Google maps ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](./Grounding.ipynb#maps_grounding) section of the grounding notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65_qu3UsM8_M"
      },
      "source": [
        "<a name=\"youtube_link\"></a>\n",
        "### Process a YouTube link"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXNlrAsZR7bB"
      },
      "source": [
        "For YouTube links, you don't need to explicitly upload the video file content, but you do need to explicitly declare the video URL you want the model to process as part of the `contents` of the request. For more information see the [documentation](https://ai.google.dev/gemini-api/docs/video-understanding#youtube) including the features and limits.\n",
        "\n",
        "> **Note:** You're only able to submit up to one YouTube link per `generate_content` request.\n",
        "\n",
        "> **Note:** If your text input includes YouTube links, the system won't process them, which may result in incorrect responses. To ensure proper handling, explicitly provide the URL using the `file_uri` parameter in `FileData`.\n",
        "\n",
        "The following example shows how you can use the model to summarize the video. In this case use a summary video of [Google I/O 2025](\"https://www.youtube.com/watch?v=LxvErFkBXPk\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcqXUrYSTrLQ"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents= types.Content(\n",
        "        parts=[\n",
        "            types.Part(text=\"Summarize this video of Google I/O 2025.\"),\n",
        "            types.Part(\n",
        "                file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=LxvErFkBXPk')\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyEmvUJfMkCR"
      },
      "source": [
        "<a name=\"URL_context\"></a>\n",
        "### Use URL context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr36dvTWQSAJ"
      },
      "source": [
        "The URL Context tool empowers Gemini models to directly access, process, and understand content from user-provided web page URLs. This is key for enabling dynamic agentic workflows, allowing models to independently research, analyze articles, and synthesize information from the web as part of their reasoning process.\n",
        "\n",
        "In this example you will use two links as reference and ask Gemini to find differences between the cook receipes present in each of the links:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgpP6_qTQeXR"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "  Compare recipes from https://www.food.com/recipe/homemade-cream-of-broccoli-soup-271210\n",
        "  and from https://www.allrecipes.com/recipe/13313/best-cream-of-broccoli-soup/,\n",
        "  list the key differences between them.\n",
        "\"\"\"\n",
        "\n",
        "tools = []\n",
        "tools.append(types.Tool(url_context=types.UrlContext))\n",
        "\n",
        "client = genai.Client(api_key=GEMINI_API_KEY)\n",
        "config = types.GenerateContentConfig(\n",
        "    tools=tools,\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "      contents=[prompt],\n",
        "      model=MODEL_ID,\n",
        "      config=config\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rl-y9SZywD0s"
      },
      "source": [
        "<a name=\"function_calling\"></a>\n",
        "## Function calling\n",
        "\n",
        "[Function calling](https://ai.google.dev/gemini-api/docs/function-calling) lets you provide a set of tools that it can use to respond to the user's prompt. You create a description of a function in your code, then pass that description to a language model in a request. The response from the model includes:\n",
        "- The name of a function that matches the description.\n",
        "- The arguments to call it with."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbng1zqyhNbU"
      },
      "source": [
        "More details and examples in the [function calling guide ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](./Function_calling.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APk6sXO6wLQp"
      },
      "outputs": [],
      "source": [
        "get_destination = types.FunctionDeclaration(\n",
        "    name=\"get_destination\",\n",
        "    description=\"Get the destination that the user wants to go to\",\n",
        "    parameters={\n",
        "        \"type\": \"OBJECT\",\n",
        "        \"properties\": {\n",
        "            \"destination\": {\n",
        "                \"type\": \"STRING\",\n",
        "                \"description\": \"Destination that the user wants to go to\",\n",
        "            },\n",
        "        },\n",
        "    },\n",
        ")\n",
        "\n",
        "destination_tool = types.Tool(\n",
        "    function_declarations=[get_destination],\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"I'd like to travel to Paris.\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=[destination_tool],\n",
        "        temperature=0,\n",
        "        ),\n",
        ")\n",
        "\n",
        "response.candidates[0].content.parts[0].function_call"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z2FJtzzHB2g"
      },
      "source": [
        "You can also use [MCP servers](https://ai.google.dev/gemini-api/docs/function-calling#mcp)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsNd3DtDFX1X"
      },
      "source": [
        "<a name=\"code_execution\"></a>\n",
        "## Code execution\n",
        "\n",
        "[Code execution](https://ai.google.dev/gemini-api/docs/code-execution?lang=python) lets the model generate and execute Python code to answer complex questions.\n",
        "\n",
        "You can find more examples in the [Code execution guide  ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](./Code_execution.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fY062-nsGLBu"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, Markdown, Code, HTML\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Generate and run a script to count how many letter r there are in the word strawberry\",\n",
        "    config = types.GenerateContentConfig(\n",
        "        tools=[types.Tool(code_execution=types.ToolCodeExecution)]\n",
        "    )\n",
        ")\n",
        "\n",
        "for part in response.candidates[0].content.parts:\n",
        "  if part.text is not None:\n",
        "    display(Markdown(part.text))\n",
        "  if part.executable_code is not None:\n",
        "    code_html = f'<pre style=\"background-color: green;\">{part.executable_code.code}</pre>'\n",
        "    display(HTML(code_html))\n",
        "  if part.code_execution_result is not None:\n",
        "    display(Markdown(part.code_execution_result.output))\n",
        "  if part.inline_data is not None:\n",
        "    display(Image(data=part.inline_data.data, format=\"png\"))\n",
        "  display(Markdown(\"---\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTgeR3_9wN5J"
      },
      "source": [
        "<a name=\"caching\"></a>\n",
        "## Use context caching\n",
        "\n",
        "[Context caching](https://ai.google.dev/gemini-api/docs/caching?lang=python) lets you to store frequently used input tokens in a dedicated cache and reference them for subsequent requests, eliminating the need to repeatedly pass the same set of tokens to a model. You can find more caching examples [in the dedicated guide ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](./Caching.ipynb).\n",
        "\n",
        "Note that for models older than 2.5, you needed to use fixed version models (often ending with `-001`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tgl2gzmuwQXz"
      },
      "source": [
        "#### Create a cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2Jb0gaiwOVi"
      },
      "outputs": [],
      "source": [
        "system_instruction = \"\"\"\n",
        "  You are an expert researcher who has years of experience in conducting systematic literature surveys and meta-analyses of different topics.\n",
        "  You pride yourself on incredible accuracy and attention to detail. You always stick to the facts in the sources provided, and never make up new facts.\n",
        "  Now look at the research paper below, and answer the following questions in 1-2 sentences.\n",
        "\"\"\"\n",
        "\n",
        "urls = [\n",
        "    'https://storage.googleapis.com/cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf',\n",
        "    \"https://storage.googleapis.com/cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrZ64o5Sydm_"
      },
      "outputs": [],
      "source": [
        "# Download files\n",
        "pdf_bytes = requests.get(urls[0]).content\n",
        "pdf_path = pathlib.Path('2312.11805v3.pdf')\n",
        "pdf_path.write_bytes(pdf_bytes)\n",
        "\n",
        "pdf_bytes = requests.get(urls[1]).content\n",
        "pdf_path = pathlib.Path('2403.05530.pdf')\n",
        "pdf_path.write_bytes(pdf_bytes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrylX7r3w2bF"
      },
      "outputs": [],
      "source": [
        "# Upload the PDFs using the File API\n",
        "uploaded_pdfs = []\n",
        "uploaded_pdfs.append(client.files.upload(file='2312.11805v3.pdf'))\n",
        "uploaded_pdfs.append(client.files.upload(file='2403.05530.pdf'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MBsaipow7m5"
      },
      "outputs": [],
      "source": [
        "# Create a cache with a 60 minute TTL\n",
        "cached_content = client.caches.create(\n",
        "    model=MODEL_ID,\n",
        "    config=types.CreateCachedContentConfig(\n",
        "      display_name='research papers', # used to identify the cache\n",
        "      system_instruction=system_instruction,\n",
        "      contents=uploaded_pdfs,\n",
        "      ttl=\"3600s\",\n",
        "  )\n",
        ")\n",
        "\n",
        "cached_content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2870527e1c84"
      },
      "source": [
        "#### Listing available cache objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3be7c2339be3"
      },
      "outputs": [],
      "source": [
        "for cache in client.caches.list():\n",
        "  print(cache)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKgCRRXfwU_m"
      },
      "source": [
        "#### Use a cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Qo7-sU2w92j"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "  model=MODEL_ID,\n",
        "  contents=\"What is the research goal shared by these research papers?\",\n",
        "  config=types.GenerateContentConfig(cached_content=cached_content.name)\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QSOsWurx4CG"
      },
      "source": [
        "#### Delete a cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSZeG60Dx4V-"
      },
      "outputs": [],
      "source": [
        "result = client.caches.delete(name=cached_content.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXNCRn8Wx71d"
      },
      "source": [
        "<a name=\"embeddings\"></a>\n",
        "## Get text embeddings\n",
        "\n",
        "You can get text embeddings for a snippet of text by using `embed_content` method and using the `gemini-embedding-001` model.\n",
        "\n",
        "The Gemini Embeddings model produces an output with 3072 dimensions by default. However, you've the option to choose an output dimensionality between 1 and 3072. See the [embeddings documentation](https://ai.google.dev/gemini-api/docs/embeddings) or the [dedicated notebook ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](./Embeddings.ipynb)\n",
        "for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpJIA5zmx8Vy"
      },
      "outputs": [],
      "source": [
        "TEXT_EMBEDDING_MODEL_ID = \"gemini-embedding-001\" # @param [\"gemini-embedding-001\", \"text-embedding-004\"] {\"allow-input\":true, isTemplate: true}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0afi69R9x_bh"
      },
      "outputs": [],
      "source": [
        "response = client.models.embed_content(\n",
        "    model=TEXT_EMBEDDING_MODEL_ID,\n",
        "    contents=[\n",
        "        \"How do I get a driver's license/learner's permit?\",\n",
        "        \"How do I renew my driver's license?\",\n",
        "        \"How do I change my address on my driver's license?\"\n",
        "        ],\n",
        ")\n",
        "\n",
        "print(response.embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tje8pMbd5z7j"
      },
      "source": [
        "You'll get a set of three embeddings, one for each piece of text you passed in:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNeuBlNt4CRk"
      },
      "outputs": [],
      "source": [
        "len(response.embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG5UPmq3543E"
      },
      "source": [
        "You can also see the length of each embedding is 3072, The default size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4oAtC8a4GYH"
      },
      "outputs": [],
      "source": [
        "print(len(response.embeddings[0].values))\n",
        "print((response.embeddings[0].values[:4], '...'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aOFsyrnlczP"
      },
      "source": [
        "<a name=\"gemini3\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7pdO-4mlhZs"
      },
      "source": [
        "## Gemini 3\n",
        "\n",
        "[Gemini 3 Pro](https://ai.google.dev/gemini-api/docs/models#gemini-3-pro) and [Gemini 3 Flash](https://ai.google.dev/gemini-api/docs/models#gemini-3-flash) are our new flagship models that comes with a few new features.\n",
        "\n",
        "The main one is the [thinking levels](#thinking_level) that simplifies how to control the amount of thinking your model does. The [Media resolution](#media_resolution) lets you control the quality of the images and videos that will be sent to the model. Finallly, the \"Thought Signatures\" are helping it maintain reasoning context across API calls.\n",
        "\n",
        "Also note that a temperature of 1 is recommended for this model generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "I0Gt67OplkxZ"
      },
      "outputs": [],
      "source": [
        "# @title Run this cell to set everything up (especially if you jumped directly to this section)from google.colab import userdata\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from IPython.display import display, Markdown, HTML\n",
        "\n",
        "client = genai.Client(api_key=userdata.get('GEMINI_API_KEY'))\n",
        "\n",
        "# Select the Gemini 3 model\n",
        "\n",
        "GEMINI_3_MODEL_ID = \"gemini-3-flash-preview\" # @param [\"gemini-3-flash-preview\", \"gemini-3-pro-preview\"] {\"allow-input\":true, isTemplate: true}\n",
        "\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/data/jetpack.png -O jetpack.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUBeYvL70O4y"
      },
      "source": [
        "<a name=\"thinking_level\"></a>\n",
        "### Thinking levels\n",
        "\n",
        "Instead of using a `thinking_budget` like the 2.5 generation (cf. [thinking](#thinking) section earlier), the third generation of Gemini models uses \"Thinking levels\" to make it simpler to manage.\n",
        "\n",
        "You can set that thinking level to \"minimal\" (more or less equivalent to \"off\"), \"low\", \"medium\" or \"high\" (default). This will indicate to the model if it allowed to do a lot of thinking. Since the thinking process stays dynamic, `high` doesn't mean it will always use a lot of token in its thinking phase, just that it's allowed to. Note that Gemini 3 Pro only supports \"low\" and \"high\".\n",
        "\n",
        "`thinking_budget` is still supported by Gemini 3 models.\n",
        "\n",
        "Check the [thinking guide ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](./Get_started_thinking.ipynb#gemini3) or the [Gemini 3  documentation](https://ai.google.dev/gemini-api/docs/gemini-3) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5t_YHsJDLi0E"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "  Find what I'm thinking of:\n",
        "    It moves, but doesn't walk, run, or swim.\n",
        "    It has no fixed shape and if cut into pieces, those pieces will keep living and moving.\n",
        "    It has no brain but can solve complex mazes.\n",
        "\"\"\"\n",
        "\n",
        "# Thinking levels can be either \"Minimal/Low/Medium/High\" or types.ThinkingLevel.MINIMAL/types.ThinkingLevel.LOW/types.ThinkingLevel.MEDIUM/types.ThinkingLevel.HIGH\n",
        "thinking_level = \"High\" # @param [\"Minimal\", \"Low\", \"Medium\",\"High\"]\n",
        "\n",
        "response = client.models.generate_content(\n",
        "  model=GEMINI_3_MODEL_ID,\n",
        "  contents=prompt,\n",
        "  config=types.GenerateContentConfig(\n",
        "    thinking_config=types.ThinkingConfig(\n",
        "      thinking_level=thinking_level,\n",
        "      include_thoughts=True\n",
        "    )\n",
        "  )\n",
        ")\n",
        "\n",
        "for part in response.parts:\n",
        "  if not part.text:\n",
        "    continue\n",
        "  if part.thought:\n",
        "    display(Markdown(\"### Thought summary:\"))\n",
        "    display(Markdown(part.text))\n",
        "    print()\n",
        "  else:\n",
        "    display(Markdown(\"### Answer:\"))\n",
        "    display(Markdown(part.text))\n",
        "    print()\n",
        "\n",
        "print(f\"We used {response.usage_metadata.thoughts_token_count} tokens for the thinking phase and {response.usage_metadata.prompt_token_count} for the output.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRkqdIRESGOy"
      },
      "source": [
        "<a name=\"media_resolution\"></a>\n",
        "### Media resolution per file\n",
        "\n",
        "With Gemini 3 models, you can specify a media resolution for image and PDF inputs, which affects how images are tokenized and how many tokens are used for each image. This can be controlled **per file**.\n",
        "\n",
        "Here are what the different values corresponds to for images and PDFs:\n",
        "* `MEDIA_RESOLUTION_HIGH`: 1120 tokens\n",
        "* `MEDIA_RESOLUTION_MEDIUM`: 560 tokens\n",
        "* `MEDIA_RESOLUTION_LOW`: 280 tokens\n",
        "* `MEDIA_RESOLUTION_UNSPECIFIED` (default): Same as `MEDIA_RESOLUTION_HIGH` for images, and `MEDIA_RESOLUTION_MEDIUM` for PDFs.\n",
        "\n",
        "For videos, `MEDIA_RESOLUTION_LOW` and `MEDIA_RESOLUTION_MEDIUM` corresponds to 70 tokens per frame, while `MEDIA_RESOLUTION_HIGH` will send 280 tokens per frame.\n",
        "\n",
        "Note that these are maximums, and the actual token usage will usually be slightly lower (by approx 10%).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIUO5FJiM4Ug"
      },
      "outputs": [],
      "source": [
        "# Media resolution is only available with `v1alpha`.\n",
        "client = genai.Client(\n",
        "    api_key=userdata.get('GEMINI_API_KEY'),\n",
        "    http_options={\n",
        "        'api_version': 'v1alpha',\n",
        "    }\n",
        ")\n",
        "\n",
        "# Upload to File API\n",
        "sample_image = client.files.upload(file=\"jetpack.png\")\n",
        "\n",
        "media_resolution = 'MEDIA_RESOLUTION_HIGH' # @param ['MEDIA_RESOLUTION_UNSPECIFIED','MEDIA_RESOLUTION_LOW','MEDIA_RESOLUTION_MEDIUM','MEDIA_RESOLUTION_HIGH']\n",
        "# You can also use types.PartMediaResolutionLevel.MEDIA_RESOLUTION_LOW/MEDIUM/HIGH\n",
        "\n",
        "count_tokens_response = client.models.count_tokens(\n",
        "    model=GEMINI_3_MODEL_ID,\n",
        "    contents=[\n",
        "        types.Part(\n",
        "            file_data=types.FileData(\n",
        "                file_uri=sample_image.uri,\n",
        "                mime_type=sample_image.mime_type\n",
        "            ),\n",
        "            media_resolution=types.PartMediaResolution(\n",
        "                level=media_resolution\n",
        "            ),\n",
        "        )\n",
        "    ],\n",
        ")\n",
        "print(f\"The image is worth {count_tokens_response.total_tokens} tokens.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tv2Fgd2572Z"
      },
      "source": [
        "### Thoughts signatures\n",
        "\n",
        "This new addidtion won't affect you if you are using the SDK since it's entirerly managed by the SDKs. But if you are curious, here's what happening behind the scenes.\n",
        "\n",
        "If you check your response's part, you'll notice a new addition: a `thought_signature`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzxMHUlB-odd"
      },
      "outputs": [],
      "source": [
        "print(response.parts[1].thought_signature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8YltZ_lzJv6"
      },
      "source": [
        "This signature is used by the model when you want to do chat/multi-turn discussions. It helps the model not only remember what was said before, but also what it thought before or what it got from its tools and function calls.\n",
        "\n",
        "Here's a example: imagine you ask the model for the temperature today. It will do a tool call or use google search to get the weather then it will tell you that it will be 25 degres. If you then ask what the humidity is, it will be able to remember that it also got that info from the first call and not do a new request."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqXH0pDz-fi2"
      },
      "source": [
        "More details on the [documentation](https://ai.google.dev/gemini-api/docs/gemini-3?thinking=explicit#thought_signatures)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4ddrPesOLTd"
      },
      "source": [
        "<a name=\"gemini3migration\"></a>\n",
        "### Migrating from Gemini 2.5\n",
        "\n",
        "[Gemini 3](https://ai.google.dev/gemini-api/docs/gemini-3) models are our most capable model family to date and offers a stepwise improvement over Gemini 2.5 Pro. When migrating, consider the following:\n",
        "\n",
        "* **Thinking:** If you were previously using complex prompt engineering (like Chain-of-thought) to force Gemini 2.5 to reason, try Gemini 3 with [`thinking_level: \"high\"`](#thinking_level) and simplified prompts (more in the [thinking](./Get_started_thinking.ipynb#gemini3migration) guide.   \n",
        "* **Temperature settings:** If your existing code explicitly sets temperature (especially to low values for deterministic outputs), we recommend removing this parameter and using the Gemini 3 default of 1.0 to avoid potential looping issues or performance degradation on complex tasks.  \n",
        "* **PDF & document understanding:** Default OCR resolution for PDFs has changed. If you relied on specific behavior for dense document parsing, test the new [`MEDIA_RESOLUTION_HIGH`](#media_resolution) setting to ensure continued accuracy.  \n",
        "* **Token consumption:** Migrating to Gemini 3 Pro defaults may **increase** token usage for PDFs but **decrease** token usage for video. If requests now exceed the context window due to higher default resolutions, we recommend explicitly reducing the [media resolution](#media_resolution).   \n",
        "* **Image segmentation:** Image segmentation capabilities (returning pixel-level masks for objects) are not supported in Gemini 3 Pro. For workloads requiring native image segmentation, we recommend continuing to utilize Gemini 2.5 Flash with thinking turned off (cf. [Spatial understanding guide ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](./Spatial_understanding.ipynb)) or [Gemini Robotics-ER 1.5 ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](./gemini-robotics-er.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SOkIVJIyF1W"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "### Useful API references:\n",
        "\n",
        "Check out the [Google GenAI SDK](https://github.com/googleapis/python-genai) and its [documentation](https://googleapis.github.io/python-genai/) for more details on the GenAI SDK.\n",
        "\n",
        "### Related examples\n",
        "\n",
        "For more detailed examples using Gemini models, check the [Quickstarts folder of the cookbook](https://github.com/google-gemini/cookbook/tree/main/quickstarts/).\n",
        "\n",
        "You'll learn how to use the [Live API ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](./Get_started_LiveAPI.ipynb), juggle with [multiple tools ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](../examples/LiveAPI_plotting_and_mapping.ipynb) or use Gemini's [spatial understanding ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](./Spatial_understanding.ipynb) abilities.\n",
        "\n",
        "You should also check out all the gen-media models:\n",
        " * Podcast and speech generation using [Gemini TTS ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](./Get_started_TTS.ipynb),\n",
        "* Live interaction with [Gemini Live ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](./Get_started_LiveAPI.ipynb),\n",
        "* Image generation using [Imagen ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](./Get_started_imagen.ipynb),\n",
        "* Video generation using [Veo ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](./Get_started_Veo.ipynb),\n",
        "* Music generation using [Lyria RealTime ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](./Get_started_LyriaRealTime.ipynb).\n",
        "\n",
        "Then, head to the [Gemini thinking models ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](./Get_started_thinking.ipynb) guide that explicitly showcases its thoughts summaries and can manage more complex reasonings.\n",
        "\n",
        "Finally, have a look at the [examples](https://github.com/google-gemini/cookbook/tree/main/examples/) folder of the cookbook for more complex use-cases and demos mixing different capabilities."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Get_started.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}